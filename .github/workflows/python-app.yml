name: Python application with LLM Bug Analysis (New Repo)

on:
  push:
    branches: [ "main", "feature/**" ] # è§¦å‘ main å’Œ feature åˆ†æ”¯æ¨é€
  pull_request:
    branches: [ "main" ] # è§¦å‘æŒ‡å‘ main çš„ PR

permissions:
  contents: read
  pull-requests: write # éœ€è¦å†™æƒé™æ¥æ·»åŠ è¯„è®º

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4 # ä½¿ç”¨ v4 ç‰ˆæœ¬

    - name: Set up Python 3.11
      uses: actions/setup-python@v5 # ä½¿ç”¨ v5 ç‰ˆæœ¬
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt # å®‰è£…ä¾èµ–

    # Linting step (optional but good practice)
    # - name: Lint with flake8
    #   run: |
    #     pip install flake8
    #     flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
    #     flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Test with pytest and capture output # Debug: Removed redirection
      id: run_tests
      continue-on-error: true
      run: |
        echo "--- Preparing to run pytest ---"
        export PYTHONPATH=$PYTHONPATH:$(pwd)
        echo "PYTHONPATH set. Running pytest..."
        
        python -m pytest tests/ --verbose
        exit_code=$? # Capture exit code directly
        
        echo "--- Pytest finished with exit code: $exit_code ---"
        echo "tests_exit_code=$exit_code" >> $GITHUB_OUTPUT
        
        echo "--- Checking exit code to set tests_failed output ---"
        if [ $exit_code -ne 0 ]; then
          echo "Exit code is non-zero ($exit_code). Tests failed! Setting tests_failed=true"
          echo "tests_failed=true" >> $GITHUB_OUTPUT
        else
          echo "Exit code is zero. Tests passed! Setting tests_failed=false"
          echo "tests_failed=false" >> $GITHUB_OUTPUT
        fi
        echo "--- Finished setting outputs for run_tests step ---"

    - name: Extract failing test code
      # åªåœ¨æµ‹è¯•å¤±è´¥æ—¶è¿è¡Œ
      if: steps.run_tests.outputs.tests_failed == 'true'
      id: extract_code
      run: |
        failed_test_line="${{ steps.run_tests.outputs.failed_test }}" # e.g., FAILED tests/test_example_bugs.py::TestExampleBugs::test_append_to_list_bug
        # æå–æµ‹è¯•æ–‡ä»¶è·¯å¾„
        test_file=$(echo "$failed_test_line" | awk '{print $2}' | cut -d: -f1) # e.g., tests/test_example_bugs.py

        if [ -z "$test_file" ] || [ ! -f "$test_file" ]; then
          echo "Could not extract test file path from '$failed_test_line' or file not found."
          echo "source_code=Could not find related source code" >> $GITHUB_OUTPUT
          exit 0
        fi

        # å°è¯•æ‰¾åˆ°å¯¹åº”çš„æºæ–‡ä»¶ (å»æ‰ test_ å‰ç¼€ï¼ŒæŸ¥æ‰¾æ ¹ç›®å½•æˆ– src/)
        base_name=$(basename "$test_file" .py) # e.g., test_example_bugs

        source_code="Could not find related source code" # Default message
        source_file_path=""

        if [[ "$base_name" == test_* ]]; then
            impl_name="${base_name#test_}.py" # e.g., example_bugs.py

            # åœ¨æ ¹ç›®å½•æŸ¥æ‰¾
            if [ -f "$impl_name" ]; then
                source_file_path="$impl_name"
                echo "Found implementation file: $source_file_path"
            # å¯é€‰ï¼šå¦‚æœå°†æ¥ä»£ç ç§»åˆ° src/ï¼Œåˆ™åœ¨æ­¤å¤„æ·»åŠ æŸ¥æ‰¾ src/ çš„é€»è¾‘
            # elif [ -f "src/$impl_name" ]; then
            #    source_file_path="src/$impl_name"
            #    echo "Found implementation file: $source_file_path"
            else
                echo "Implementation file '$impl_name' not found in root directory."
                # Fallback: å°è¯•åœ¨æ ¹ç›®å½•æ‰¾åå­—ç›¸å…³çš„ .py æ–‡ä»¶ï¼ˆä¸å¤ªç²¾ç¡®ï¼‰
                # possible_impl_name="${impl_name%.py}"
                # related_file=$(find . -maxdepth 1 -name "*.py" -not -path '*/test_*.py' -exec grep -l "$possible_impl_name" {} + | head -n 1)
                # if [ -f "$related_file" ]; then
                #     source_file_path="$related_file"
                #     echo "Found related file: $source_file_path"
                # fi
            fi
        else
            echo "Test filename '$base_name' does not follow 'test_*' convention."
             # å¦‚æœæµ‹è¯•æ–‡ä»¶åä¸è§„èŒƒï¼Œå°è¯•ç›´æ¥ç”¨æµ‹è¯•æ–‡ä»¶åï¼ˆå»æ‰è·¯å¾„ï¼‰æ‰¾åŒåæºæ–‡ä»¶
            plain_base_name=$(basename "$test_file")
            if [ -f "$plain_base_name" ]; then
               source_file_path="$plain_base_name"
               echo "Found source file with same name as test file (excluding path): $source_file_path"
            fi
        fi

        # å¦‚æœæ‰¾åˆ°äº†æºæ–‡ä»¶è·¯å¾„ï¼Œè¯»å–å†…å®¹
        if [ -n "$source_file_path" ] && [ -f "$source_file_path" ]; then
             source_code=$(cat "$source_file_path")
             echo "source_file=$source_file_path" >> $GITHUB_OUTPUT
        else
             echo "Source file path not determined or file does not exist."
             echo "source_file=unknown" >> $GITHUB_OUTPUT
        fi

        # è¾“å‡ºæºä»£ç å†…å®¹
        echo "source_code<<EOF" >> $GITHUB_OUTPUT
        echo "$source_code" >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT


    - name: Analyze with Open Router LLM
      # åªåœ¨æµ‹è¯•å¤±è´¥ä¸”æ‰¾åˆ°æºä»£ç æ—¶è¿è¡Œ
      if: steps.run_tests.outputs.tests_failed == 'true' && steps.extract_code.outputs.source_code != 'Could not find related source code'
      id: llm_analysis
      env:
        # ç¡®ä¿åœ¨ä»“åº“ Settings -> Secrets -> Actions ä¸­è®¾ç½®äº†æ­¤å¯†é’¥
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      run: |
        # åˆ›å»ºè°ƒç”¨ OpenRouter çš„ Python è„šæœ¬
        cat > call_openrouter.py << 'EOF'
        import os
        import sys
        import json
        import requests

        api_key = os.environ.get("OPENROUTER_API_KEY")
        if not api_key:
            print(json.dumps({"error": "Open Router API key (OPENROUTER_API_KEY) is not set in GitHub Secrets."}))
            sys.exit(1)

        # ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è·å–è¾“å…¥
        test_output = os.environ.get("TEST_OUTPUT")
        source_code = os.environ.get("SOURCE_CODE")
        source_file = os.environ.get("SOURCE_FILE")

        if not all([test_output, source_code, source_file]):
             print(json.dumps({"error": "Missing input data (TEST_OUTPUT, SOURCE_CODE, or SOURCE_FILE)."}))
             sys.exit(1)

        prompt = f"""
        A Python test failed in my project. Please analyze the following test output and the related source code to suggest a fix.

        ## Test Output
        ```
        {test_output}
        ```

        ## Related Source Code File ({source_file})
        ```python
        {source_code}
        ```

        Please provide:
        1. An analysis of the root cause of the test failure.
        2. Specific suggestions for fixing the code, including the corrected code snippet.
        3. An explanation of how the fix addresses the problem.

        Format the response in Markdown.
        """

        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}",
            "HTTP-Referer": "https://github.com/YOUR_USERNAME/YOUR_REPO_NAME", # Replace with your details or keep generic
            "X-Title": "GitHub Actions LLM Bug Analyzer"
        }
        payload = {
            "model": "anthropic/claude-3-haiku-20240307", # Using Haiku for speed/cost
            "messages": [
                {"role": "system", "content": "You are an expert Python programmer specializing in debugging and fixing code based on failing tests. Provide clear, concise, and accurate analysis and solutions."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,
            "max_tokens": 1500
        }

        try:
            response = requests.post(url, headers=headers, json=payload, timeout=120) # Added timeout
            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

            result = response.json()
            if "choices" in result and len(result["choices"]) > 0 and "message" in result["choices"][0] and "content" in result["choices"][0]["message"]:
               suggestion = result["choices"][0]["message"]["content"]
               print(json.dumps({"suggestion": suggestion}))
            else:
               print(json.dumps({"error": f"Unexpected response format from OpenRouter: {result}"}))

        except requests.exceptions.RequestException as e:
            print(json.dumps({"error": f"Error calling OpenRouter API: {e}"}))
        except Exception as e:
            print(json.dumps({"error": f"An unexpected error occurred: {e}"}))

        EOF

        # æ‰§è¡Œè„šæœ¬ï¼Œå°†ä¸Šä¸‹æ–‡ä½œä¸ºç¯å¢ƒå˜é‡ä¼ é€’
        export TEST_OUTPUT="${{ steps.run_tests.outputs.test_output }}"
        export SOURCE_CODE="${{ steps.extract_code.outputs.source_code }}"
        export SOURCE_FILE="${{ steps.extract_code.outputs.source_file }}"
        python_output=$(python call_openrouter.py)

        # æ£€æŸ¥ python_output æ˜¯å¦ä¸ºç©ºæˆ–è¡¨ç¤ºé”™è¯¯
        if [ -z "$python_output" ]; then
          python_output=$(json.dumps({"error": "Python script produced no output."}))
        fi

        # ä¿å­˜ LLM è¾“å‡º
        echo "llm_output<<EOF" >> $GITHUB_OUTPUT
        echo "$python_output" >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT

    - name: Add PR comment with fix suggestion
      # åªåœ¨æµ‹è¯•å¤±è´¥ã€LLM åˆ†ææˆåŠŸä¸”åœ¨ PR ä¸Šä¸‹æ–‡è¿è¡Œæ—¶è¿è¡Œ
      if: steps.run_tests.outputs.tests_failed == 'true' && steps.llm_analysis.outputs.llm_output != '' && github.event_name == 'pull_request'
      uses: actions/github-script@v7 # ä½¿ç”¨ v7 ç‰ˆæœ¬
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }} # ä½¿ç”¨é»˜è®¤çš„ GITHUB_TOKEN
        script: |
          const llmOutputJson = `${{ steps.llm_analysis.outputs.llm_output }}`;
          let llmOutput;
          try {
            llmOutput = JSON.parse(llmOutputJson);
          } catch (e) {
            console.error("Error parsing LLM output:", e);
            console.error("Raw LLM output:", llmOutputJson);
            // Optionally add a comment indicating the parsing error
            github.rest.issues.createComment({
               owner: context.repo.owner,
               repo: context.repo.repo,
               issue_number: context.issue.number,
               body: `## ğŸ¤– AI Analysis Error\n\nFailed to parse the response from the LLM analysis step. Please check the workflow logs.`
            });
            return; // Stop execution for this step
          }

          // æ£€æŸ¥ LLM è¾“å‡ºä¸­æ˜¯å¦æœ‰é”™è¯¯å­—æ®µ
          if (llmOutput.error) {
            console.log(`LLM analysis step reported an error: ${llmOutput.error}`);
            // æ·»åŠ åŒ…å«é”™è¯¯çš„è¯„è®º
            github.rest.issues.createComment({
               owner: context.repo.owner,
               repo: context.repo.repo,
               issue_number: context.issue.number,
               body: `## ğŸ¤– AI Analysis Error\n\nThe LLM analysis step failed with the following error:\n\`\`\`\n${llmOutput.error}\n\`\`\`\nPlease check the workflow logs and ensure the OPENROUTER_API_KEY secret is correctly configured.`
            });
            return; // Stop execution for this step
          }

          // å¦‚æœæ²¡æœ‰é”™è¯¯ï¼Œæå–å»ºè®®å¹¶åˆ›å»ºè¯„è®º
          const suggestion = llmOutput.suggestion || "No suggestion provided."; // Fallback if suggestion is missing
          const sourceFile = `${{ steps.extract_code.outputs.source_file }}` || "unknown file"; // Fallback

          const body = `## ğŸ¤– AI Test Failure Analysis & Fix Suggestion\n\nI detected a test failure related to \`${sourceFile}\` and performed an analysis. Here's the suggestion:\n\n${suggestion}\n\n---\nâš ï¸ **Note**: This is an AI-generated suggestion. Please review it carefully before applying.`;

          // åˆ›å»ºè¯„è®º
          try {
             await github.rest.issues.createComment({
               owner: context.repo.owner,
               repo: context.repo.repo,
               // context.issue.number åªåœ¨ PR äº‹ä»¶ä¸­æœ‰æ•ˆ
               issue_number: context.issue.number,
               body: body
             });
          } catch (error) {
             console.error(`Error creating PR comment: ${error}`);
             // Maybe the issue number wasn't available (e.g., run on push, not PR)
             console.log('Could not create PR comment. This might be expected if the workflow was not triggered by a pull request.');
          } 